---
title: "aske_rebecca_experiment2"
author: "Aske & Rebecca"
date: "7 October 2020"
output: word_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



## Gather data
Load packages, data, wd etc. 

```{r}

# packages
pacman::p_load(brms, tidyverse,lmerTest,ggbeeswarm, lme4, rethinking, bayesplot)

citation("rethinking")
citation("bayesplot")

# WD
setwd("data")
```



Collect data from several csv files

```{r}
# import data from several csv files
filePaths2 <- list.files("data/logfiles-aesthetics/", "\\.csv$", full.names = TRUE)
dfa <- do.call(rbind, lapply(filePaths2, read.csv))


# Dummy coding two new response columns:
dfa <- dfa %>% 
  mutate(
    ResponseL = ifelse(Response == "left", 1, 0),
    ResponseR = ifelse(Response == "right", 1, 0)
    )



dfa <- dfa %>%    # xxx decide whether or not to have capital first letters
  mutate(
    ID = as.factor(ID),
    Age = as.factor(Age),
    Gender = as.factor(Gender),
    StimulusL = as.factor(StimulusL), #In original it says "left" (is this the correct alternative? xxx)
    StimulusR = as.factor(StimulusR), #In original it says "right" (is this the correct alternative? xxx)
    ResponseL = as.factor(ResponseL), #xxx Do they have these dummy coded? update: I did it, is it correct?
    ResponseR = as.factor(ResponseR), 
    ComplexityL = as.factor(ComplexityL),
    ComplexityR= as.factor(ComplexityR),
    NoiseL = as.factor(NoiseL),
    NoiseR= as.factor(NoiseR),
    BlankSpaceL = as.factor(BlankSpaceL),
    BlankSpaceR= as.factor(BlankSpaceR),
    OreintationL = as.factor(OrientationL),
    OreintationR = as.factor(OrientationR),
    # Creating ID for stim pair xxx
    StimPair = as.factor(paste0(StimulusL, StimulusR)),
   
    reaction_time = reaction_time
  )


# Create csv in shared folder
write.csv(dfa,"data/exp2_dfa.csv", row.names = FALSE)
```




## Preprocess data
Load packages, wd, and data

```{r}
# packages
pacman::p_load(tidyverse,lmerTest,ggbeeswarm, lme4, rethinking, brms)

# import data from csv - Lærke: I guess this isn't actually necessary for me, but here goes:
dfa <- read_csv("data/exp2_dfa.csv")
```



Create stimPair column (look in one of the other scripts)

# xxx This is mostly in order except from the nrows (see xxx). 23.11.21 update: I think this may not be relevant for me? xxx Figure out this stimpairID thing. Do i need stimpair as random effects? do I need to do what they did?
```{r}

# make empty dataframe
dfa_new <- dfa[0,] 


# create count
count = 1 


# the LOOP - add StimPairID column
for (group in 1:length(unique(dfa$chainPair))){
  
  ## for each chainpair loop through and create StimPair unique ID
  sub <- subset(dfa, chainPair == group)

  # Create list of target pictures in the chain group (all 24 pics are represented in at least one participant)
  stimuli1 <- unique(sub$left) %>% as.data.frame()
  stimuli2 <- unique(sub$right) %>% as.data.frame()
  stimuli3 <- rbind(stimuli1, stimuli2)

  colnames(stimuli3)[1] <- "stim"

  stimuli <- unique(stimuli3$stim)

  x <- NULL
  x <- data.frame(matrix(unlist(combn(stimuli, 2, simplify=F)), nrow=276, byrow=T)) # 276 = (24*24-24)/2 #xxx how do i do this step, when i have a different number of observations for each participant? can i use this: "nrow = table(dfa$ID)" instead of 276?
  x$StimPairID <- seq(nrow(x))


  # Loop adding stimPair value to the current subset
  sub$StimPairID <- NA

  
  for (i in seq(nrow(x))){
    sub$StimPairID[
      (sub$left %in% x$X1[i] | sub$left %in% x$X2[i]) & 
      (sub$right %in% x$X1[i] | sub$right %in% x$X2[i])] = x$StimPairID[i]
  }

  
  # Add value so they all become unique across stim groups
  sub$StimPairID <- sub$StimPairID + (1000 * count)
  count <- count + 1

  
  # Combine with premade empty dataframe
  if (nrow(dfa_new) == 0) {
    dfa_new <- sub
    } else {
        dfa_new <- rbind(dfa_new, sub)
  }
}

```



remove trials of same generation forced choice (xxx Generation must mean period. So in my case, I don't want forced choice between for example two c1n1b1 condition type stimuli. That is a bit more unlikely in my case though, because I have waaay more combinations.). 23.11.21 update: we want to create subsets for each compositional dimension (noise, complexity, and bs) where we remove trials of the same dimension. I.e. we don't want to see a choice between two complexity level 2's.

```{r}

# Complexity subset:
dfa_c <- dfa %>% subset(ComplexityL != ComplexityR)

# Noise subset:
dfa_n <- dfa %>% subset(NoiseL != NoiseR)

# Blank space subset:
dfa_b <- dfa %>% subset(BlankSpaceL != BlankSpaceR)
```


Create ‘Distance’ column based on generationL  (xxx so: Create ‘Distance’ column based on ComplexityL, one based on noiseL, and one based on BlankSpaceL, or should I make one single column for all three of them?) 23.11.21 update: in the complexity subset (dfa_c) i make a complexity distance column, and so on.

```{r}

# relevel as factor
dfa_c$ComplexityL <- factor(dfa_c$ComplexityL, levels = c(1, 2, 3))
dfa_c$ComplexityR <- factor(dfa_c$ComplexityR, levels = c(1, 2, 3))

dfa_n$NoiseL <- factor(dfa_n$NoiseL, levels = c(1, 2, 3))
dfa_n$NoiseR <- factor(dfa_n$NoiseR, levels = c(1, 2, 3))

dfa_b$BlankSpaceL <- factor(dfa_b$BlankSpaceL, levels = c(1, 2, 3))
dfa_b$BlankSpaceR <- factor(dfa_b$BlankSpaceR, levels = c(1, 2, 3))



# when transformed from leveled factor; becomes 1, 2, and 3. (xxx don't know if this is true/necessary for me when my levels were already 1,2, and 3)
dfa_c$DistanceC = as.numeric(dfa_c$ComplexityL)
dfa_n$DistanceN = as.numeric(dfa_n$NoiseL)
dfa_b$DistanceB = as.numeric(dfa_b$BlankSpaceL)



# subtract right from left
dfa_c$DistanceC <- dfa_c$DistanceC - as.numeric(dfa_c$ComplexityR) #xxx
dfa_n$DistanceN <- dfa_n$DistanceN - as.numeric(dfa_n$NoiseR)      #xxx
dfa_b$DistanceB <- dfa_b$DistanceB - as.numeric(dfa_b$BlankSpaceR) #xxx make this ordinal

class(dfa_c$DistanceC)

# Change variable before running models

#dfa_c$StimPairID <- as.factor(dfa_c$StimPairID)
dfa_c$StimPair <- as.factor(dfa_c$StimPair)
dfa_c$ResponseL <- as.factor(dfa_c$ResponseL)
dfa_c$ID <- as.factor(dfa_c$ID)

#dfa_n$StimPairID <- as.factor(dfa_n$StimPairID)
dfa_n$StimPair <- as.factor(dfa_n$StimPair)
dfa_n$ResponseL <- as.factor(dfa_n$ResponseL)
dfa_n$ID <- as.factor(dfa_n$ID)

#dfa_b$StimPairID <- as.factor(dfa_b$StimPairID)
dfa_b$StimPair <- as.factor(dfa_b$StimPair)
dfa_b$ResponseL <- as.factor(dfa_b$ResponseL)
dfa_b$ID <- as.factor(dfa_b$ID)

```



## Modelling

```{r the Null Model}
# define chains, iter, and controls
CHAINS = 2
CORES = 2
ITER = 4000   #23.11.21: Kristian suggested 4000 (originally it said 1000)

CONTROLS = list(
  max_treedepth = 20,
  adapt_delta=0.99)


# make subset
#sub <- c(6, 7, 8, 9, 10) #xxx what is this?


# Bernoulli is the likelihood function  --> outcome is the log odds(?) xxx what?



### MODEL 0

# Construct models
a_c_f0 <- bf(ResponseL ~ 1) #  + (1 + distance | ID) + (1 | StimPairID)


# get priors to be set
get_prior(a_c_m0, dfa, family = bernoulli()) #xxx now that I have three different subset df's do i then also have to run three different null models? because they run on different data? or do I just run the null model on the big data?


# set priors
a_c_prior0 <- c(
  prior(normal(0, 1.5), class = Intercept) #xxx find your own priors, once you get the df up and running. The get_prior just gives me the generic student's t. where do these priors come from?
)


p <- rnorm(10000, 0, 1.5) # p = distribution. xxx Change numbers?

dens(p) # density plot (log odds)

dens(inv_logit(p)) # probabiloity for rate --> this is a nice prior (xxx check on my data). 23.11.21 update: on my data it looks pretty nicely normal/gaussian distributed (I think)



# Run model based on priors alone
a_c_m0_prior <- brm(
   a_c_f0,
   #data = subset(dfa, ID %in% sub), #xxx not sure about the data to use. why are they using that weird little subset?
   data = dfa_c,
   family = bernoulli(),
   prior = a_c_prior0,
   sample_prior = "only"
)


# prior predictive check
pp_check(a_c_m0_prior, nsamples=100) #is this good? i'm not sure


## Better pp_check
y_pred <- posterior_linpred(a_c_m0_prior) # generate predictions from the model, but we don't want 0's and 1's , we want which rates are expected

# we want linear predictions before it is linked into log-odds. 
dens(inv_logit(y_pred)) # looks at the density now. Almost uniform distribution discounting extremes.



# Run model
a_c_m0 <- brm(
   a_c_f0,
   data = subset(dfa, ID %in% sub),
   family = bernoulli(),
   prior = prior0,
   sample_prior = TRUE
)



summary(a_c_m0)



# prior predictive check
pp_check(a_c_m0, nsamples=100)



## Better pp_check
y_pred1 <- posterior_linpred(a_c_m0)  
dens(inv_logit(y_pred1))
```


 
```{r Complexity models}

#the model formulas: 
#xxx null model ...
a_c_f1 <- bf(ResponseL ~ 1 + DistanceC + (1 + DistanceC | ID)) #orientation?
a_c_f2 <- bf(ResponseL ~ 1 + DistanceC + I(DistanceC)^2 + (1 + DistanceC + I(DistanceC)^2 | ID))


# get priors to be set
#xxx null model priors ...
get_prior(a_c_f1, dfa_c, family = bernoulli())
get_prior(a_c_f2, dfa_c, family = bernoulli())


# defining the priors: (xxx are priors supposed to be the same for both models? If yes, then just delete one of these)
a_c_prior <- c(       # xxx find your own priors
  prior(normal(0,1.5), class = Intercept),
  prior(normal(0,0.5), class = b), #This would be increased in order to make the quadratic model post less constrained.
  prior(normal(0,0.3), class = sd), 
  prior(lkj(5), class = cor)
)

# Run prior models
#xxx null model prior model ...

a_c_m1_prior <- brm(
   a_c_f1,
   data = dfa_c, #subset(d2, ID %in% sub)
   family = bernoulli(),
   prior = a_c_prior,
   sample_prior = "only"
)

a_c_m2_prior <- brm(
   a_c_f2,
   data = dfa_c,
   family = bernoulli(),
   prior = a_c_prior,
   sample_prior = "only"
)


# prior predictive checks
#xxx null model prior model pp checks ...

pp_check(a_c_m1_prior, nsamples=100)
y_pred1 <- posterior_linpred(a_c_m1_prior)  
dens(inv_logit(y_pred1))

pp_check(a_c_m2_prior, nsamples=100)
y_pred2 <- posterior_linpred(a_c_m2_prior)  
dens(inv_logit(y_pred2))



# Run models
# xxx null model ...

a_c_m1 <- brm(          #Model ran very fast and is not at all significant
   a_c_f1,
   data = dfa_c,
   family = bernoulli(),
   prior = a_c_prior,
   sample_prior = TRUE,
   chains = CHAINS,
   cores=CORES,
   iter = ITER,
   file = "models/aesthetics/a_c_m1"
)

a_c_m2 <- brm(
   a_c_f2,
   data = dfa_c,
   family = bernoulli(),
   prior = a_c_prior,
   sample_prior = TRUE,
   chains = CHAINS,
   cores=CORES,
   iter = ITER,
   file = "models/aesthetics/a_c_m2"
)


# prior predictive checks
#pp_check(a_c_m1, nsamples=100)
#y_pred1 <- posterior_linpred(a_c_m1)  
#dens(inv_logit(y_pred1))

#pp_check(a_c_m2, nsamples=100)
#y_pred1 <- posterior_linpred(a_c_m2)  
#dens(inv_logit(y_pred1))


## Model results

# Model 1
summary(a_c_m1)
hypothesis(a_c_m1,"DistanceC > 0")  #what exactly does this mean?

exp(0.45) # chance          xxx do i need to change these numbers?
inv_logit(0.45 + 0.28) # probability
inv_logit(0.45) 

# remember trace plots etc.
plot(a_c_m1)

# hypothesis testing
plot(hypothesis(a_c_m1,"DistanceC > 0"))
hypothesis(a_c_m1,"DistanceC > 0")


# Model 2
summary(a_c_m2)
hypothesis(a_c_m2,"DistanceC > 0")  #what exactly does this mean?

#exp() # chance              xxx what are these?
#inv_logit() # probability

# remember trace plots etc.
plot(a_c_m2)

# hypothesis testing
plot(hypothesis(a_c_m2,"DistanceC > 0"))
hypothesis(a_c_m2,"DistanceC > 0")

plot(a_c_m2)


#model 1
print(summary(a_c_m1))
print(marginal_effects(a_c_m1))
print(hypothesis(a_c_m1, "DistanceC < 0", class = "b"))
plot(hypothesis(a_c_m1, "DistanceC < 0", class = "b")) #the more complex the less aesthetic. the prob of choosing left goes up when it is lower level of complexity than right. 
#model 2
print(summary(a_c_m2))
print(marginal_effects(a_c_m2))
print(hypothesis(a_c_m2, "DistanceC < 0", class = "b"))
plot(hypothesis(a_c_m2, "DistanceC < 0", class = "b"))# this would be fixed by altering sd prior
```

```{r Noise models}

#the model formulas: 
#xxx null model ...
a_n_f1 <- bf(ResponseL ~ 1 + DistanceN + (1 + DistanceN | ID)) #orientation?
a_n_f2 <- bf(ResponseL ~ 1 + DistanceN + I(DistanceN)^2 + (1 + DistanceN + I(DistanceN)^2 | ID))


# get priors to be set
#xxx null model priors ...
get_prior(a_n_f1, dfa_n, family = bernoulli())
get_prior(a_n_f2, dfa_n, family = bernoulli())


# defining the priors: (xxx are priors supposed to be the same for both models? If yes, then just delete one of these)
a_n_prior <- c(       # xxx find your own priors
  prior(normal(0,1.5), class = Intercept),
  prior(normal(0,0.5), class = b),
  prior(normal(0,0.3), class = sd),
  prior(lkj(5), class = cor)
)

# Run prior models
#xxx null model prior model ...

a_n_m1_prior <- brm(
   a_n_f1,
   data = dfa_n, #subset(d2, ID %in% sub)
   family = bernoulli(),
   prior = a_n_prior,
   sample_prior = "only"
)

a_n_m2_prior <- brm(
   a_n_f2,
   data = dfa_n,
   family = bernoulli(),
   prior = a_n_prior,
   sample_prior = "only"
)


# prior predictive checks
#xxx null model prior model pp checks ...

pp_check(a_n_m1_prior, nsamples=100)
y_pred1 <- posterior_linpred(a_n_m1_prior)  
dens(inv_logit(y_pred1))

pp_check(a_n_m2_prior, nsamples=100)
y_pred2 <- posterior_linpred(a_n_m2_prior)  
dens(inv_logit(y_pred2))



# Run models
# xxx null model ...

a_n_m1 <- brm(          #Model ran very fast and is not at all significant
   a_n_f1,
   data = dfa_n,
   family = bernoulli(),
   prior = a_n_prior,
   sample_prior = TRUE,
   chains = CHAINS,
   cores=CORES,
   iter = ITER,
   file = "models/aesthetics/a_n_m1"
)

a_n_m2 <- brm(
   a_n_f2,
   data = dfa_n,
   family = bernoulli(),
   prior = a_n_prior,
   sample_prior = TRUE,
   chains = CHAINS,
   cores=CORES,
   iter = ITER,
   file = "models/aesthetics/a_n_m2"
)

## Model results



#model 1
print(summary(a_n_m1))
print(marginal_effects(a_n_m1))
print(hypothesis(a_n_m1, "DistanceN < 0", class = "b"))
plot(hypothesis(a_n_m1, "DistanceN < 0", class = "b"))
plot(a_n_m1)
#model 2
print(summary(a_n_m2))
print(marginal_effects(a_n_m2))
print(hypothesis(a_n_m2, "DistanceN < 0", class = "b"))
plot(hypothesis(a_n_m2, "DistanceN < 0", class = "b"))
plot(a_n_m2)
```

```{r Blank space models}

#the model formulas: 
#xxx null model ...
a_b_f1 <- bf(ResponseL ~ 1 + DistanceB + (1 + DistanceB | ID)) #orientation?
a_b_f2 <- bf(ResponseL ~ 1 + DistanceB + I(DistanceB)^2 + (1 + DistanceB + I(DistanceB)^2 | ID))


# get priors to be set
#xxx null model priors ...
get_prior(a_b_f1, dfa_b, family = bernoulli())
get_prior(a_b_f2, dfa_b, family = bernoulli())


# defining the priors: (xxx are priors supposed to be the same for both models? If yes, then just delete one of these)
a_b_prior <- c(       # xxx find your own priors
  prior(normal(0,1.5), class = Intercept),
  prior(normal(0,0.5), class = b),
  prior(normal(0,0.3), class = sd),
  prior(lkj(5), class = cor)
)

# Run prior models
#xxx null model prior model ...

a_b_m1_prior <- brm(
   a_b_f1,
   data = dfa_b, #subset(d2, ID %in% sub)
   family = bernoulli(),
   prior = a_b_prior,
   sample_prior = "only"
)

a_b_m2_prior <- brm(
   a_b_f2,
   data = dfa_b,
   family = bernoulli(),
   prior = a_b_prior,
   sample_prior = "only"
)


# prior predictive checks
#xxx null model prior model pp checks ...

pp_check(a_b_m1_prior, nsamples=100)
y_pred1 <- posterior_linpred(a_b_m1_prior)  
dens(inv_logit(y_pred1))

pp_check(a_b_m2_prior, nsamples=100)
y_pred2 <- posterior_linpred(a_b_m2_prior)  
dens(inv_logit(y_pred2))



# Run models
# xxx null model ...

a_b_m1 <- brm(          #Model ran very fast and is not at all significant
   a_b_f1,
   data = dfa_b,
   family = bernoulli(),
   prior = a_b_prior,
   sample_prior = TRUE,
   chains = CHAINS,
   cores=CORES,
   iter = ITER,
   file = "models/aesthetics/a_b_m1"
)

a_b_m2 <- brm(
   a_b_f2,
   data = dfa_b,
   family = bernoulli(),
   prior = a_b_prior,
   sample_prior = TRUE,
   chains = CHAINS,
   cores=CORES,
   iter = ITER,
   file = "models/aesthetics/a_b_m2"
)

## Model results
#model 1
print(summary(a_b_m1))
print(marginal_effects(a_b_m1))
print(hypothesis(a_b_m1, "DistanceB > 0", class = "b"))
plot(hypothesis(a_b_m1, "DistanceB > 0", class = "b"))
plot(a_b_m1)
#model 2
print(summary(a_b_m2))
print(marginal_effects(a_b_m2))
print(hypothesis(a_b_m2, "DistanceB > 0", class = "b"))
plot(hypothesis(a_b_m2, "DistanceB > 0", class = "b"))
plot(a_b_m1)

#for raw data plor change responseL to numeric
```



Model comparison

```{r}



###

a_c_m0 <- add_criterion(a_c_m0, criterion = c("bayes_R2", "loo"))

a_c_m1 <- add_criterion(a_c_m1, criterion = c("bayes_R2", "loo"))

a_c_m2 <- add_criterion(a_c_m2, criterion = c("bayes_R2", "loo"))





loo_compare(a_c_m1, a_c_m2) # number tries to estimate out of sample error. Baelines to 0. Best model is 0.

loo_model_weights(a_c_m1, a_c_m2)

```


## Model comparisons

```{r}

# model contrlling for pics

m2.3 <- add_criterion(m2.3, criterion = c("bayes_R2", "loo"))



# models controlling for chain on all data

m2.5 <- add_criterion(m2.5, criterion = c("bayes_R2", "loo"))

m2.6 <- add_criterion(m2.6, criterion = c("bayes_R2", "loo"))



# models controlling for chain on all data

m2.7 <- add_criterion(m2.7, criterion = c("bayes_R2", "loo"))

m2.8 <- add_criterion(m2.8, criterion = c("bayes_R2", "loo"))







# first models

exp2_compare1 <- loo_compare(m2.1, m2.2) 

exp2_weights1 <- loo_model_weights(m2.1,m2.2)

exp2_compare1

exp2_weights1 # most simple model is better





# control models for chain with all data

exp2_compare2 <- loo_compare(m2.5, m2.6) 

exp2_weights2 <- loo_model_weights(m2.5,m2.6)

exp2_compare2

exp2_weights2 # interaction control model is better





# compare all models on all data

exp2_compare3 <- loo_compare(m2.1, m2.2, m2.5, m2.6) 

exp2_weights3 <- loo_model_weights(m2.1, m2.2, m2.5, m2.6)

exp2_compare3

exp2_weights3 # Control model with interaction effect was found most credible



# compare best first model with the two control chain models

exp2_compare4 <- loo_compare(m2.1, m2.5, m2.6) 

exp2_weights4 <- loo_model_weights(m2.1, m2.5, m2.6)

exp2_compare4

exp2_weights4 #comparison without quadratic model --> again 2.6 is by far the best model



# control models for chain without prob pics

exp2_compare5 <- loo_compare(m2.7, m2.8) 

exp2_weights5 <- loo_model_weights(m2.7,m2.8)

exp2_compare5

exp2_weights5



# compare with the first not controlling for chain

exp2_compare6 <- loo_compare(m2.3, m2.7, m2.8) 

exp2_weights6 <- loo_model_weights(m2.3, m2.7, m2.8)

exp2_compare6 

exp2_weights6





summary(m2.1)

summary(m2.2)

summary(m2.3)

summary(m2.4)

summary(m2.5)

summary(m2.6)

summary(m2.7)

summary(m2.8)





```


# Reporting the best model

```{r}



# both controlling for problematic pictures and chain group

summary(m2.8)

hypothesis(m2.8, "distance:chainD1 > 0")



# Relative effect scale, ignore baserate

exp(0.51) #  the odds of selecting the left stimulus increase 67% when the predictor ‘distance’ increases one unit



#absolute effect, take base rate into account (xxx what is the base rate?)

inv_logit(0.37)

inv_logit(0.37 + 0.51)

inv_logit(0.37 + 0.51) - inv_logit(0.37) # A positive unit change in the predictor elicits a 11.5% increase in probability of choosing the left stimulus





# great effect

hypothesis(m2.8,"distance > 0")



# no effects

hypothesis(m2.8,"chainD1 > 0")

hypothesis(m2.8,"distance:chainD1 > 0")



# plot results

plot(hypothesis(m2.8,"distance > 0")) # the priors do not seem too sceptical as the data does not try to escape it...



plot(m2.8)



conditional_effects(m2.8)



```


## Plots

```{r}



# Change response L back to numeric

dfa_c$ResponseL <- as.numeric(as.character(dfa_c$ResponseL))



# Create summary dataset for visualisation

plotSum <- dfa_c %>% group_by(ID, DistanceC) %>% summarise(

  LeftChoice = mean(ResponseL)

)





library(pacman)

p_load(extrafont)

font_import(pattern="[T/t]imes")

loadfonts(device="win")



Exp2_MainPlot <- ggplot(plotSum, aes(DistanceC, LeftChoice)) + 

  geom_line(aes(group=ID,color=ID),alpha=0.6) +

  geom_point(aes(group=ID,color=ID),alpha=0.6)+

  geom_smooth(method=lm, color = "red") +

  scale_color_discrete(guide=FALSE) +

  scale_x_continuous(breaks = c(-2, -1, 0, 1, 2),labels=c("-2","-1", "", "1", "2")) +

  

  # THEMES

  theme_grey() +

  theme(

    text = element_text(family = "Times New Roman"),

    legend.position="top",

    plot.subtitle=element_text(face="italic",size=14,colour="grey40"),

    plot.title=element_text(size=21,face="bold")) +

  labs(title="\nExperiment 2",

       subtitle="Intentionality",

       x=expression("Generation distance from right to left"),

       y=expression("Rate of choosing the left stimulus")) +

  NULL

  

Exp2_MainPlot 
#individual differences
```


```{r}



# Change response L back to numeric

dfa_n$ResponseL <- as.numeric(as.character(dfa_n$ResponseL))



# Create summary dataset for visualisation

plotSum <- dfa_n %>% group_by(ID, DistanceN) %>% summarise(

  LeftChoice = mean(ResponseL)

)





Exp2_MainPlot2 <- ggplot(plotSum, aes(DistanceN, LeftChoice)) + 

  geom_line(aes(group=ID,color=ID),alpha=0.6) +

  geom_point(aes(group=ID,color=ID),alpha=0.6)+

  geom_smooth(method=lm, color = "red") +

  scale_color_discrete(guide=FALSE) +

  scale_x_continuous(breaks = c(-2, -1, 0, 1, 2),labels=c("-2","-1", "", "1", "2")) +

  

  # THEMES

  theme_grey() +

  theme(

    text = element_text(family = "Times New Roman"),

    legend.position="top",

    plot.subtitle=element_text(face="italic",size=14,colour="grey40"),

    plot.title=element_text(size=21,face="bold")) +

  labs(title="\nExperiment 2",

       subtitle="Intentionality",

       x=expression("Generation distance from right to left"),

       y=expression("Rate of choosing the left stimulus")) +

  NULL

  

Exp2_MainPlot2
#individual differences
```



```{r}



# Change response L back to numeric

dfa_b$ResponseL <- as.numeric(as.character(dfa_b$ResponseL))



# Create summary dataset for visualisation

plotSum <- dfa_b %>% group_by(ID, DistanceB) %>% summarise(

  LeftChoice = mean(ResponseL)

)





Exp2_MainPlot3 <- ggplot(plotSum, aes(DistanceB, LeftChoice)) + 

  geom_line(aes(group=ID,color=ID),alpha=0.6) +

  geom_point(aes(group=ID,color=ID),alpha=0.6)+

  geom_smooth(method=lm, color = "red") +

  scale_color_discrete(guide=FALSE) +

  scale_x_continuous(breaks = c(-2, -1, 0, 1, 2),labels=c("-2","-1", "", "1", "2")) +

  

  # THEMES

  theme_grey() +

  theme(

    text = element_text(family = "Times New Roman"),

    legend.position="top",

    plot.subtitle=element_text(face="italic",size=14,colour="grey40"),

    plot.title=element_text(size=21,face="bold")) +

  labs(title="\nExperiment 2",

       subtitle="Intentionality",

       x=expression("Generation distance from right to left"),

       y=expression("Rate of choosing the left stimulus")) +

  NULL

  

Exp2_MainPlot3
#individual differences
```



```{r}
ggplot(dfa_c, aes(DistanceC, as.numeric(ResponseL))) + 
  geom_point() +
  geom_smooth()
class(dfa_c$DistanceC)
```



# running all remaining models
```{r Delete this - models have been copy pasted}
a_n_f1 <- bf(ResponseL ~ 1 + DistanceN + (1 + DistanceN | ID))
a_n_f2 <- bf(ResponseL ~ 1 + DistanceN + I(DistanceN)^2 + (1 + DistanceN + I(DistanceN)^2 | ID))
a_b_f1 <- bf(ResponseL ~ 1 + DistanceB + (1 + DistanceB | ID))
a_b_f2 <- bf(ResponseL ~ 1 + DistanceB + I(DistanceB)^2 + (1 + DistanceB + I(DistanceB)^2 | ID))


a_n_prior <- c(
  prior(normal(0,1.5), class = Intercept),
  prior(normal(0,0.5), class = b),
  prior(normal(0,0.3), class = sd),
  prior(lkj(5), class = cor)
)
a_b_prior <- c(
  prior(normal(0,1.5), class = Intercept),
  prior(normal(0,0.5), class = b),
  prior(normal(0,0.3), class = sd),
  prior(lkj(5), class = cor)
)

a_n_m1 <- brm(
   a_n_f1,
   data = dfa_n,
   family = bernoulli(),
   prior = a_n_prior,
   sample_prior = TRUE,
   chains = CHAINS,
   cores=CORES,
   iter = ITER,
   file = "models/aesthetics/a_n_m1"
)
a_n_m2 <- brm(
   a_n_f2,
   data = dfa_n,
   family = bernoulli(),
   prior = a_n_prior,
   sample_prior = TRUE,
   chains = CHAINS,
   cores=CORES,
   iter = ITER,
   file = "models/aesthetics/a_n_m2"
)
a_b_m1 <- brm(    
   a_b_f1,
   data = dfa_b,
   family = bernoulli(),
   prior = a_b_prior,
   sample_prior = TRUE,
   chains = CHAINS,
   cores=CORES,
   iter = ITER,
   file = "models/aesthetics/a_b_m1"
)
a_b_m2 <- brm(
   a_b_f2,
   data = dfa_b,
   family = bernoulli(),
   prior = a_b_prior,
   sample_prior = TRUE,
   chains = CHAINS,
   cores=CORES,
   iter = ITER,
   file = "models/aesthetics/a_b_m2"
)
######################################################

m_n_f1 <- bf(MSE_1 ~ 1 + Noise_numeric + (1 + Noise_numeric | ID) + (1 | Stimulus) + (1 | Orientation))
m_n_f1_q <- bf(MSE_1 ~ 1 + Noise_numeric + I(Noise_numeric)^2 
               (1 + Noise_numeric + I(Noise_numeric)^2 | ID) + 
                 (1 | Stimulus) + 
                 (1 | Orientation))

m_n_f2 <- bf(MSE_1 ~ 1 + Noise_ordered + (1 + Noise_ordered | ID) + (1 | Stimulus) + (1 | Orientation)) 
m_n_f2.1 <- bf(MSE_1 ~ 1 + Noise_ordered + (1 + Noise_ordered | ID) + (1 | Stimulus)) 
m_n_f2.2 <- bf(MSE_1 ~ 1 + Noise_ordered * Orientation + (1 + Noise_ordered | ID) + (1 | Stimulus)) 

m_n_prior1 <- c(
  prior(normal(0,.3), class = Intercept),
  prior(normal(0,.1), class = b),
  prior(normal(0,.05), class = sd),
  prior(lkj(5), class = cor)
)

# model
m_n_m1 <- brm(m_n_f1,
             data = dfm,
             family = gaussian,
             prior = m_n_prior1,
             sample_prior = TRUE,
                   chains = CHAINS,cores=CHAINS,
                   iter = ITER,
                   control = CONTROLS,
             file = "models/memorability/m_n_m1"
             )

m_n_m1_q <- brm(m_n_f1,
             data = dfm,
             family = gaussian,
             prior = m_n_prior1,
             sample_prior = TRUE,
                   chains = CHAINS,cores=CHAINS,
                   iter = ITER,
                   control = CONTROLS,
             file = "models/memorability/m_n_m1_q"
             )

m_n_m2 <- brm(m_n_f2,
             data = dfm,
             family = gaussian,
             prior = m_c_prior1,
             sample_prior = TRUE,
                   chains = CHAINS,cores=CHAINS,
                   iter = ITER,
                   control = CONTROLS,
             file = "models/memorability/m_n_m2"
             )

m_n_m2.1 <- brm(m_n_f2.1,
             data = dfm,
             family = gaussian,
             prior = m_c_prior1,
             sample_prior = TRUE,
                   chains = CHAINS,cores=CHAINS,
                   iter = ITER,
                   control = CONTROLS,
             file = "models/memorability/m_n_m2.1"
             )

m_n_m2.2 <- brm(m_n_f2.2,
             data = dfm,
             family = gaussian,
             prior = m_c_prior1,
             sample_prior = TRUE,
                   chains = CHAINS,cores=CHAINS,
                   iter = ITER,
                   control = CONTROLS,
             file = "models/memorability/m_n_m2.2"
             )

#########################################
m_b_f1 <- bf(MSE_1 ~ 1 + BlankSpace_numeric + (1 + BlankSpace_numeric | ID) + (1 | Stimulus) + (1 | Orientation))
m_b_f1_q <- bf(MSE_1 ~ 1 + BlankSpace_numeric + I(BlankSpace_numeric)^2 
               (1 + BlankSpace_numeric + I(BlankSpace_numeric)^2 | ID) + 
                 (1 | Stimulus) + 
                 (1 | Orientation))

m_b_f2 <- bf(MSE_1 ~ 1 + BlankSpace_ordered + (1 + BlankSpace_ordered | ID) + (1 | Stimulus) + (1 | Orientation)) 
m_b_f2.1 <- bf(MSE_1 ~ 1 + BlankSpace_ordered + (1 + BlankSpace_ordered | ID) + (1 | Stimulus)) 
m_b_f2.2 <- bf(MSE_1 ~ 1 + BlankSpace_ordered * Orientation + (1 + BlankSpace_ordered | ID) + (1 | Stimulus)) 

m_b_prior1 <- c(
  prior(normal(0,.3), class = Intercept),
  prior(normal(0,.1), class = b),
  prior(normal(0,.05), class = sd),
  prior(lkj(5), class = cor)
)

# model
m_b_m1 <- brm(m_b_f1,
             data = dfm,
             family = gaussian,
             prior = m_b_prior1,
             sample_prior = TRUE,
                   chains = CHAINS,cores=CHAINS,
                   iter = ITER,
                   control = CONTROLS,
             file = "models/memorability/m_b_m1"
             )

m_b_m1_q <- brm(m_b_f1,
             data = dfm,
             family = gaussian,
             prior = m_b_prior1,
             sample_prior = TRUE,
                   chains = CHAINS,cores=CHAINS,
                   iter = ITER,
                   control = CONTROLS,
             file = "models/memorability/m_b_m1_q"
             )

m_b_m2 <- brm(m_b_f2,
             data = dfm,
             family = gaussian,
             prior = m_b_prior1,
             sample_prior = TRUE,
                   chains = CHAINS,cores=CHAINS,
                   iter = ITER,
                   control = CONTROLS,
             file = "models/memorability/m_b_m2"
             )

m_b_m2.1 <- brm(m_b_f2.1,
             data = dfm,
             family = gaussian,
             prior = m_b_prior1,
             sample_prior = TRUE,
                   chains = CHAINS,cores=CHAINS,
                   iter = ITER,
                   control = CONTROLS,
             file = "models/memorability/m_b_m2.1"
             )

m_b_m2.2 <- brm(m_b_f2.2,
             data = dfm,
             family = gaussian,
             prior = m_b_prior1,
             sample_prior = TRUE,
                   chains = CHAINS,cores=CHAINS,
                   iter = ITER,
                   control = CONTROLS,
             file = "models/memorability/m_b_m2.2"
             )
```



