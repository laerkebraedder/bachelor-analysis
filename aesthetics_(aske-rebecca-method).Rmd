---
title: "aske_rebecca_experiment2"
author: "Aske & Rebecca"
date: "7 October 2020"
output: word_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



## Gather data
Load packages, data, wd etc. 

```{r}

# packages
pacman::p_load(brms, tidyverse,lmerTest,ggbeeswarm, lme4, rethinking, bayesplot)

citation("rethinking")
citation("bayesplot")

# WD
setwd("data")
```



Collect data from several csv files

```{r}
# import data from several csv files
filePaths2 <- list.files("data/logfiles-aesthetics/", "\\.csv$", full.names = TRUE)
dfa <- do.call(rbind, lapply(filePaths2, read.csv))


# Dummy coding two new response columns:
dfa <- dfa %>% 
  mutate(
    ResponseL = ifelse(Response == "left", 1, 0),
    ResponseR = ifelse(Response == "right", 1, 0)
    )



dfa <- dfa %>%    # xxx decide whether or not to have capital first letters
  mutate(
    ID = as.factor(ID),
    Age = as.factor(Age),
    Gender = as.factor(Gender),
    StimulusL = as.factor(StimulusL), #In original it says "left" (is this the correct alternative? xxx)
    StimulusR = as.factor(StimulusR), #In original it says "right" (is this the correct alternative? xxx)
    ResponseL = as.factor(ResponseL), #xxx Do they have these dummy coded? update: I did it, is it correct?
    ResponseR = as.factor(ResponseR), 
    ComplexityL = as.factor(ComplexityL),
    ComplexityR= as.factor(ComplexityR),
    NoiseL = as.factor(NoiseL),
    NoiseR= as.factor(NoiseR),
    BlankSpaceL = as.factor(BlankSpaceL),
    BlankSpaceR= as.factor(BlankSpaceR),
    OreintationL = as.factor(OrientationL),
    OreintationR = as.factor(OrientationR),
    # Creating ID for stim pair xxx
    StimPair = as.factor(paste0(StimulusL, StimulusR)),
   
    reaction_time = reaction_time
  )


# Create csv in shared folder
write.csv(dfa,"data/exp2_dfa.csv", row.names = FALSE)
```




## Preprocess data
Load packages, wd, and data

```{r}
# packages
pacman::p_load(tidyverse,lmerTest,ggbeeswarm, lme4, rethinking, brms)

# import data from csv - Lærke: I guess this isn't actually necessary for me, but here goes:
dfa <- read_csv("data/exp2_dfa.csv")
```



Create stimPair column (look in one of the other scripts)

# xxx This is mostly in order except from the nrows (see xxx). 23.11.21 update: I think this may not be relevant for me? xxx Figure out this stimpairID thing. Do i need stimpair as random effects? do I need to do what they did?
```{r}

# make empty dataframe
dfa_new <- dfa[0,] 


# create count
count = 1 


# the LOOP - add StimPairID column
for (group in 1:length(unique(dfa$chainPair))){
  
  ## for each chainpair loop through and create StimPair unique ID
  sub <- subset(dfa, chainPair == group)

  # Create list of target pictures in the chain group (all 24 pics are represented in at least one participant)
  stimuli1 <- unique(sub$left) %>% as.data.frame()
  stimuli2 <- unique(sub$right) %>% as.data.frame()
  stimuli3 <- rbind(stimuli1, stimuli2)

  colnames(stimuli3)[1] <- "stim"

  stimuli <- unique(stimuli3$stim)

  x <- NULL
  x <- data.frame(matrix(unlist(combn(stimuli, 2, simplify=F)), nrow=276, byrow=T)) # 276 = (24*24-24)/2 #xxx how do i do this step, when i have a different number of observations for each participant? can i use this: "nrow = table(dfa$ID)" instead of 276?
  x$StimPairID <- seq(nrow(x))


  # Loop adding stimPair value to the current subset
  sub$StimPairID <- NA

  
  for (i in seq(nrow(x))){
    sub$StimPairID[
      (sub$left %in% x$X1[i] | sub$left %in% x$X2[i]) & 
      (sub$right %in% x$X1[i] | sub$right %in% x$X2[i])] = x$StimPairID[i]
  }

  
  # Add value so they all become unique across stim groups
  sub$StimPairID <- sub$StimPairID + (1000 * count)
  count <- count + 1

  
  # Combine with premade empty dataframe
  if (nrow(dfa_new) == 0) {
    dfa_new <- sub
    } else {
        dfa_new <- rbind(dfa_new, sub)
  }
}

```



remove trials of same generation forced choice (xxx Generation must mean period. So in my case, I don't want forced choice between for example two c1n1b1 condition type stimuli. That is a bit more unlikely in my case though, because I have waaay more combinations.). 23.11.21 update: we want to create subsets for each compositional dimension (noise, complexity, and bs) where we remove trials of the same dimension. I.e. we don't want to see a choice between two complexity level 2's.

```{r}

# Complexity subset:
dfa_c <- dfa %>% subset(ComplexityL != ComplexityR)

# Noise subset:
dfa_n <- dfa %>% subset(NoiseL != NoiseR)

# Blank space subset:
dfa_b <- dfa %>% subset(BlankSpaceL != BlankSpaceR)
```


Create ‘Distance’ column based on generationL  (xxx so: Create ‘Distance’ column based on ComplexityL, one based on noiseL, and one based on BlankSpaceL, or should I make one single column for all three of them?) 23.11.21 update: in the complexity subset (dfa_c) i make a complexity distance column, and so on.

```{r}

# relevel as factor
dfa_c$ComplexityL <- factor(dfa_c$ComplexityL, levels = c(1, 2, 3))
dfa_c$ComplexityR <- factor(dfa_c$ComplexityR, levels = c(1, 2, 3))

dfa_n$NoiseL <- factor(dfa_n$NoiseL, levels = c(1, 2, 3))
dfa_n$NoiseR <- factor(dfa_n$NoiseR, levels = c(1, 2, 3))

dfa_b$BlankSpaceL <- factor(dfa_b$BlankSpaceL, levels = c(1, 2, 3))
dfa_b$BlankSpaceR <- factor(dfa_b$BlankSpaceR, levels = c(1, 2, 3))



# when transformed from leveled factor; becomes 1, 2, and 3. (xxx don't know if this is true/necessary for me when my levels were already 1,2, and 3)
dfa_c$DistanceC = as.numeric(dfa_c$ComplexityL)
dfa_n$DistanceN = as.numeric(dfa_n$NoiseL)
dfa_b$DistanceB = as.numeric(dfa_b$BlankSpaceL)



# subtract right from left
dfa_c$DistanceC <- dfa_c$DistanceC - as.numeric(dfa_c$ComplexityR)
dfa_n$DistanceN <- dfa_n$DistanceN - as.numeric(dfa_n$NoiseR)
dfa_b$DistanceB <- dfa_b$DistanceB - as.numeric(dfa_b$BlankSpaceR)



# Change variable before running models

#dfa_c$StimPairID <- as.factor(dfa_c$StimPairID)
dfa_c$StimPair <- as.factor(dfa_c$StimPair)
dfa_c$ResponseL <- as.factor(dfa_c$ResponseL)
dfa_c$ID <- as.factor(dfa_c$ID)

#dfa_n$StimPairID <- as.factor(dfa_n$StimPairID)
dfa_n$StimPair <- as.factor(dfa_n$StimPair)
dfa_n$ResponseL <- as.factor(dfa_n$ResponseL)
dfa_n$ID <- as.factor(dfa_n$ID)

#dfa_b$StimPairID <- as.factor(dfa_b$StimPairID)
dfa_b$StimPair <- as.factor(dfa_b$StimPair)
dfa_b$ResponseL <- as.factor(dfa_b$ResponseL)
dfa_b$ID <- as.factor(dfa_b$ID)

```



## Modelling

```{r the Null Model}
# define chains, iter, and controls
CHAINS = 2
CORES = 2
ITER = 4000   #23.11.21: Kristian suggested 4000 (originally it said 1000)

CONTROLS = list(
  max_treedepth = 20,
  adapt_delta=0.99)


# make subset
#sub <- c(6, 7, 8, 9, 10) #xxx what is this?


# Bernoulli is the likelihood function  --> outcome is the log odds(?) xxx what?



### MODEL 0

# Construct models
a_c_f0 <- bf(ResponseL ~ 1) #  + (1 + distance | ID) + (1 | StimPairID)


# get priors to be set
get_prior(a_c_m0, dfa, family = bernoulli()) #xxx now that I have three different subset df's do i then also have to run three different null models? because they run on different data? or do I just run the null model on the big data?


# set priors
a_c_prior0 <- c(
  prior(normal(0, 1.5), class = Intercept) #xxx find your own priors, once you get the df up and running. The get_prior just gives me the generic student's t. where do these priors come from?
)


p <- rnorm(10000, 0, 1.5) # p = distribution. xxx Change numbers?

dens(p) # density plot (log odds)

dens(inv_logit(p)) # probabiloity for rate --> this is a nice prior (xxx check on my data). 23.11.21 update: on my data it looks pretty nicely normal/gaussian distributed (I think)



# Run model based on priors alone
a_c_m0_prior <- brm(
   a_c_f0,
   #data = subset(dfa, ID %in% sub), #xxx not sure about the data to use. why are they using that weird little subset?
   data = dfa_c,
   family = bernoulli(),
   prior = a_c_prior0,
   sample_prior = "only"
)


# prior predictive check
pp_check(a_c_m0_prior, nsamples=100) #is this good? i'm not sure


## Better pp_check
y_pred <- posterior_linpred(a_c_m0_prior) # generate predictions from the model, but we don't want 0's and 1's , we want which rates are expected

# we want linear predictions before it is linked into log-odds. 
dens(inv_logit(y_pred)) # looks at the density now. Almost uniform distribution discounting extremes.



# Run model
a_c_m0 <- brm(
   a_c_f0,
   data = subset(dfa, ID %in% sub),
   family = bernoulli(),
   prior = prior0,
   sample_prior = TRUE
)



summary(a_c_m0)



# prior predictive check
pp_check(a_c_m0, nsamples=100)



## Better pp_check
y_pred1 <- posterior_linpred(a_c_m0)  
dens(inv_logit(y_pred1))
```


 xxx Can I also in this analysis get away with only making one model, if I make the compositional dimensions ordered?
```{r Complexity models}

#the model formulas: 
#xxx null model ...
a_c_f1 <- bf(ResponseL ~ 1 + DistanceC + (1 + DistanceC | ID) + (1 | StimPair)) #xxx do I really need to have random effects for stimpair? seems maybe a bit silly.. (obs in the original this was stimPairID, which I am not sure what was, but just to note it.)
a_c_f2 <- bf(ResponseL ~ 1 + DistanceC + I(DistanceC)^2 + (1 + DistanceC + I(DistanceC)^2 | ID) + (1 | StimPair))


# get priors to be set
#xxx null model priors ...
get_prior(a_c_f1, dfa_c, family = bernoulli())
get_prior(a_c_f2, dfa_c, family = bernoulli())


# defining the priors: (xxx are priors supposed to be the same for both models? If yes, then just delete one of these)
a_c_prior1 <- c(       # xxx find your own priors
  prior(normal(0,1.5), class = Intercept),
  prior(normal(0,0.5), class = b),
  prior(normal(0,0.3), class = sd),
  prior(lkj(5), class = cor)
)

a_c_prior2 <- c(
  prior(normal(0,1.5), class = Intercept),
  prior(normal(0,0.3), class = b),
  prior(normal(0,0.2), class = sd),
  prior(lkj(5), class = cor)
)



# Run prior models
#xxx null model prior model ...

a_c_m1_prior <- brm(
   a_c_f1,
   data = dfa_c, #subset(d2, ID %in% sub)
   family = bernoulli(),
   prior = a_c_prior1,
   sample_prior = "only"
)

a_c_m2_prior <- brm(
   a_c_f2,
   data = dfa_c,
   family = bernoulli(),
   prior = a_c_prior2,
   sample_prior = "only"
)


# prior predictive checks
#xxx null model prior model pp checks ...

pp_check(a_c_m1_prior, nsamples=100)
y_pred1 <- posterior_linpred(a_c_m1_prior)  
dens(inv_logit(y_pred1))

pp_check(a_c_m2_prior, nsamples=100)
y_pred2 <- posterior_linpred(a_c_m2_prior)  
dens(inv_logit(y_pred2))



# Run models
# xxx null model ...

a_c_m1 <- brm(          #Model ran very fast and is not at all significant
   a_c_f1,
   data = dfa_c,
   family = bernoulli(),
   prior = a_c_prior1,
   sample_prior = TRUE,
   chains = CHAINS,
   cores=CORES,
   iter = ITER,
   file = "models/aesthetics/a_c_m1"
)

a_c_m2 <- brm(
   a_c_f2,
   data = dfa_c,
   family = bernoulli(),
   prior = a_c_prior2,
   sample_prior = TRUE,
   chains = CHAINS,
   cores=CORES,
   iter = ITER,
   file = "models/aesthetics/a_c_m2"
)


# prior predictive checks
pp_check(a_c_m1, nsamples=100)
y_pred1 <- posterior_linpred(a_c_m1)  
dens(inv_logit(y_pred1))

pp_check(a_c_m2, nsamples=100)
y_pred1 <- posterior_linpred(a_c_m2)  
dens(inv_logit(y_pred1))


## Model results

# Model 1
summary(a_c_m1)
hypothesis(a_c_m1,"DistanceC > 0")  #what exactly does this mean?

exp(0.45) # chance          xxx do i need to change these numbers?
inv_logit(0.45 + 0.28) # probability
inv_logit(0.45) 

# remember trace plots etc.
plot(a_c_m1)

# hypothesis testing
plot(hypothesis(a_c_m1,"DistanceC > 0"))
hypothesis(a_c_m1,"DistanceC > 0")


# Model 2
summary(a_c_m2)
hypothesis(a_c_m2,"DistanceC > 0")  #what exactly does this mean?

#exp() # chance              xxx what are these?
#inv_logit() # probability

# remember trace plots etc.
plot(a_c_m2)

# hypothesis testing
plot(hypothesis(a_c_m2,"DistanceC > 0"))
hypothesis(a_c_m2,"DistanceC > 0")

plot(a_c_m2)
```





Model comparison

```{r}



###

a_c_m0 <- add_criterion(a_c_m0, criterion = c("bayes_R2", "loo"))

a_c_m1 <- add_criterion(a_c_m1, criterion = c("bayes_R2", "loo"))

a_c_m2 <- add_criterion(a_c_m2, criterion = c("bayes_R2", "loo"))





loo_compare(a_c_m1, a_c_m2) # number tries to estimate out of sample error. Baelines to 0. Best model is 0.

loo_model_weights(a_c_m1, a_c_m2)

```





## Model comparisons

```{r}

# model contrlling for pics

m2.3 <- add_criterion(m2.3, criterion = c("bayes_R2", "loo"))



# models controlling for chain on all data

m2.5 <- add_criterion(m2.5, criterion = c("bayes_R2", "loo"))

m2.6 <- add_criterion(m2.6, criterion = c("bayes_R2", "loo"))



# models controlling for chain on all data

m2.7 <- add_criterion(m2.7, criterion = c("bayes_R2", "loo"))

m2.8 <- add_criterion(m2.8, criterion = c("bayes_R2", "loo"))







# first models

exp2_compare1 <- loo_compare(m2.1, m2.2) 

exp2_weights1 <- loo_model_weights(m2.1,m2.2)

exp2_compare1

exp2_weights1 # most simple model is better





# control models for chain with all data

exp2_compare2 <- loo_compare(m2.5, m2.6) 

exp2_weights2 <- loo_model_weights(m2.5,m2.6)

exp2_compare2

exp2_weights2 # interaction control model is better





# compare all models on all data

exp2_compare3 <- loo_compare(m2.1, m2.2, m2.5, m2.6) 

exp2_weights3 <- loo_model_weights(m2.1, m2.2, m2.5, m2.6)

exp2_compare3

exp2_weights3 # Control model with interaction effect was found most credible



# compare best first model with the two control chain models

exp2_compare4 <- loo_compare(m2.1, m2.5, m2.6) 

exp2_weights4 <- loo_model_weights(m2.1, m2.5, m2.6)

exp2_compare4

exp2_weights4 #comparison without quadratic model --> again 2.6 is by far the best model



# control models for chain without prob pics

exp2_compare5 <- loo_compare(m2.7, m2.8) 

exp2_weights5 <- loo_model_weights(m2.7,m2.8)

exp2_compare5

exp2_weights5



# compare with the first not controlling for chain

exp2_compare6 <- loo_compare(m2.3, m2.7, m2.8) 

exp2_weights6 <- loo_model_weights(m2.3, m2.7, m2.8)

exp2_compare6 

exp2_weights6





summary(m2.1)

summary(m2.2)

summary(m2.3)

summary(m2.4)

summary(m2.5)

summary(m2.6)

summary(m2.7)

summary(m2.8)





```





# Reporting the best model

```{r}



# both controlling for problematic pictures and chain group

summary(m2.8)

hypothesis(m2.8, "distance:chainD1 > 0")



# Relative effect scale, ignore baserate

exp(0.51) #  the odds of selecting the left stimulus increase 67% when the predictor ‘distance’ increases one unit



#absolute effect, take base rate into account (xxx what is the base rate?)

inv_logit(0.37)

inv_logit(0.37 + 0.51)

inv_logit(0.37 + 0.51) - inv_logit(0.37) # A positive unit change in the predictor elicits a 11.5% increase in probability of choosing the left stimulus





# great effect

hypothesis(m2.8,"distance > 0")



# no effects

hypothesis(m2.8,"chainD1 > 0")

hypothesis(m2.8,"distance:chainD1 > 0")



# plot results

plot(hypothesis(m2.8,"distance > 0")) # the priors do not seem too sceptical as the data does not try to escape it...



plot(m2.8)



conditional_effects(m2.8)



```





## Plots

```{r}



# Change response L back to numeric

d2$ResponseL <- as.numeric(as.character(d2$ResponseL))



# Create summary dataset for visualisation

plotSum <- d2 %>% group_by(ID, distance) %>% summarise(

  LeftChoice = mean(ResponseL)

)





library(pacman)

p_load(extrafont)

font_import(pattern="[T/t]imes")

loadfonts(device="win")



Exp2_MainPlot <- ggplot(plotSum, aes(distance, LeftChoice)) + 

  geom_line(aes(group=ID,color=ID),alpha=0.6) +

  geom_point(aes(group=ID,color=ID),alpha=0.6)+

  geom_smooth(method=lm, color = "red") +

  scale_color_discrete(guide=FALSE) +

  scale_x_continuous(breaks = c(-2, -1, 0, 1, 2),labels=c("-2","-1", "", "1", "2")) +

  

  # THEMES

  theme_grey() +

  theme(

    text = element_text(family = "Times New Roman"),

    legend.position="top",

    plot.subtitle=element_text(face="italic",size=14,colour="grey40"),

    plot.title=element_text(size=21,face="bold")) +

  labs(title="\nExperiment 2",

       subtitle="Intentionality",

       x=expression("Generation distance from right to left"),

       y=expression("Rate of choosing the left stimulus")) +

  NULL

  

Exp2_MainPlot 

```



